In this project we have gained insight into techniques used for data at scale. In particular, we investigated the problem of multi class classification using images of plant leaves. Whilst the dataset uses images of real leaves, the photographs in the Plant Village data are very consistent in the sense that they are all taken from the same angle, and have a similar background. Many datasets for real world classification problems do not benefit from the same consistency, and so our neural network results may reflect more favourable performance than can be expected for other image datasets. 

In the activation function experiment, we compared the predictive performance of four different activation functions (Swish, Relu, Tanh and Sigmoid) within a fixed neural network architecture. We found that while Swish, tanh and Relu all produced similar prediction accuracy results, Swish was over 10% slower than the other two. This study was conducted only on a very small subset of the data, and so in the case of much larger data, this difference would likely be magnified. Further, when performing data manipulation at scale, the speed at which algorithms can be trained becomes even more important. Hence it is unlikely that Swish would be a cost-effective choice for our image classification task. The Sigmoid model was both slower and yielded a worse accuracy than ReLu and Tanh. This result aligns with the literature, which suggests that the Sigmoid activation, whilst Tanh is better for multi class problems



### References
1. https://medium.com/codex/activation-functions-in-neural-network-steps-and-implementation-df2e4c858c21
